---
title: 逻辑回归的那些事
date: 2020-02-13 13:42:49
categories: 机器学习
tags: [逻辑回归, 机器学习]
mathjax: true
---
本文是笔者学习逻辑回归算法后的总结，是笔者自己对逻辑回归算法的感悟，可能有不当之处。如有疑问，欢迎提 [issue](https://github.com/BitInit/bitinit.github.io/issues)。

本文主要内容是对逻辑回归算法中代价函数(也叫损失函数，本文统一使用代价函数这词)的形成原理和梯度下降算法公式推导进行讨论，不涉及逻辑回归算法的介绍和如何使用，所以需要读者知道逻辑回归算法是什么、有什么用和怎么用。

## 写在前面
本文公式和符号采用[吴恩达教授的机器学习课程](https://www.coursera.org/learn/machine-learning)中的表达方式。以肿瘤预测为研究例子，有 m 个数据作为训练集，分别是 {$(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), (x^{(3)}, y^{(3)}), (x^{(4)}, y^{(4)}),,,(x^{(m - 1)}, y^{(m - 1)}), (x^{(m)}, y^{(m)})$}，其中 x 表示肿瘤的大小，x 的上标 i 表示第 i 个数据；y 的值为 0或1，0表示是良性肿瘤，1表示是恶性肿瘤。数据集样例如下：

x (肿瘤大小)|y (是否是恶性)
---|---
$x^{(1)}$|1
$x^{(2)}$|0
$x^{(3)}$|0
$x^{(4)}$|1
...|...
$x^{(m - 1)}$|1
$x^{(m)}$|0

## 代价函数
对于上节肿瘤预测例子，假设我们有一个函数 $f(x)$，该函数表示为：肿瘤大小为 $x$，它是恶性肿瘤的概率，用概率论的表述就是 $f(x) = P(y = 1 | x)$。

m 个测试数据集，那么我们令 $L$：

$$
L = f(x^{(1)})(1 - f(x^{(2)}))(1 - f(x^{(3)}))f(x^{(4)})...(1 - f(x^{(m - 1)}))f(x^{(m)})
$$

上述 $L$ 中，$f(x^{(2)})$表示：肿瘤大小为 $x^{(2)}$ 时，为**恶性**肿瘤的概率，但是在训练数据集中当 $x$ 为 $x^{(2)}$ 时，$y == 0$。所以 $1 - f(x^{(2)})$表示：肿瘤大小为 $x^{(2)}$ 时，为**良性**肿瘤的概率。现在可以看出 $L$ 的含义了，通过预测函数$f(x)$，生成训练集相同结果的概率。即$L$表示：有 m 个数据$x^{(1)}$、$x^{(2)}$、$x^{(3)}$...$x^{(m - 1)}$、$x^{(m)}$，我有一个预测函数$f(x)$，通过该预测函数 $f(x)$，使得肿瘤大小为$x^{(1)}$时预测为恶性肿瘤、$x^{(2)}$时预测为良性肿瘤、$x^{(3)}$时预测为良性肿瘤、...、$x^{(m - 1)}$时预测为恶性肿瘤、$x^{(m)}$时预测为良性肿瘤的概率。

有了上述的 $L$，现在我们可以来讨论 $f(x)$ 了，我们希望找到一个非常理想的 $f(x)$ 函数使得 $L$的值最大，最理想情况就是 $L$ 等于 1（当然这样会过拟合训练数据，我们这里不讨论过拟合）。这时我们的目标变成通过机器学习方法，使用训练数据集找到一个函数 $f(x)$，使得 $L$ 值最大。

通过 $f(x)$ 函数得到的 $L(f(x))$ 函数在寻找最大值是非常困难的，因为 $L(f(x))$ 函数图形是不能确定的，所以我们需要一种数学变换将 $L(f(x))$ 函数变成方便计算的函数，即把 $L(f(x))$ 变成[凸函数](https://baike.baidu.com/item/%E5%87%B8%E5%87%BD%E6%95%B0/3371735?fr=aladdin)，这里通过 $log$ 把 $L$ 转换成凸函数。

$$
J = -\frac{1}{m}log(L)
$$

式子中的 $J$ 就是逻辑回归中的**代价函数**，加上一个负号就把寻找 $L$ 最大值转换成寻找 $J$ 的最小值。$J$ 的完整表示：

$$
J = -\frac{1}{m}log[f(x^{(1)})(1 - f(x^{(2)}))(1 - f(x^{(3)}))f(x^{(4)})...(1 - f(x^{(m - 1)}))f(x^{(m)})]
$$

再变换一下：

$$
J = -\frac{1}{m}[log(f(x^{(1)})) + log(1 - f(x^{(2)})) + log(1 - f(x^{(3)})) ... - log(1 - f(x^{(m - 1)})) + log(f(x^{(m)}))]
$$

式子中的 $log(f(x^{(1)}))$ 可以写成 $1 \* log(f(x^{(1)})) + (1 - 1) \* log(1-f(x^{(1)}))$，而 $log(1 - f(x^{(2)}))$ 可以写成 $0 \* log(1 - f(x^{(2)})) + (1 - 0) \* log(1 - f(x^{(2)}))$

$x^{(3)}$、$x^{(4)}$、...、$x^{(m)}$ 同理，即可得到统一表示方式：

$$
y^{(i)} \* log(f(x^{(i)})) + (1 - y^{(i)}) \* log(1 - f(x^{(i)}))
$$

这时，代价函数 $J$ 变成：

$$
J = -\frac{1}{m}\sum_{i = 1}^{m}[y^{(i)} \* log(f(x^{(i)})) + (1 - y^{(i)}) \* log(1 - f(x^{(i)}))]
$$

以此得到我们熟悉的代价函数公式。

## 梯度下降算法
上节所说的 $f(x)$ 就是机器学习术语中的**假设函数**，在[吴恩达教授的机器学习课程](https://www.coursera.org/learn/machine-learning)中是用 $h_{\theta}(x)$ 表示。为了与课程统一，所以我们还是按照课程来定义函数：

$$
J(\theta) = -\frac{1}{m}\sum_{i = 1}^{m}[y^{(i)} \* log(h_{\theta}(x)) + (1 - y^{(i)}) \* log(1 - h_{\theta}(x))]
$$

其中 $h_{\theta}(x^{(i)}) = \frac{1}{1 + e^{-\theta^{T}x^{(i)}}}$

当进行梯度下降计算时，需要计算 $J(\theta)$ 对 $\theta$ 的偏导，偏导结论如下：

$$
\frac{\delta}{\delta\theta_{j}}J(\theta) = \frac{1}{m}\sum_{i = 1}^{m}[h_{\theta}(x^{(i)}) - y^{(i)}]x_{j}^{(i)}
$$

推导过程如下：

![推导](http://image.55555.io/blog_logistic_theta.png)